# Распределенный консенсус: от теории к практике

## Введение

Распределенный консенсус решает фундаментальную проблему достижения согласия между несколькими независимыми компьютерами по одному значению, даже если некоторые узлы могут выйти из строя или стать недоступными. Рассмотрим банковскую систему, в которой несколько серверов должны поддерживать согласованные остатки на счетах: если сервер A регистрирует 100 долларов для Алисы, а сервер B показывает 50 долларов из-за пропущенной транзакции, определение правильного остатка для снятия 75 долларов становится критической проблемой консенсуса.

Распределенный консенсус представляет собой одну из самых фундаментальных задач в информатике, лежащую в основе важнейших функций распределенных систем, включая согласованность баз данных, выбор лидера в кластерах, атомарные транзакции между службами и скоординированные изменения конфигурации.

### Сложность консенсуса

Задача консенсуса выходит за рамки простой коммуникации и включает в себя принятие решений в условиях неопределенности. Сети могут разбиваться на части, серверы могут выходить из строя в любой момент, а сообщения могут задерживаться или теряться. Системы должны одновременно поддерживать согласованность и продолжать работу, несмотря на эти проблемы.

Алгоритмы консенсуса решают эту дилемму, признавая, что ожидание универсальных ответов чревато бесконечными задержками (неисправные узлы могут никогда не ответить), а односторонние решения чревато несогласованностью (лицо, принимающее решение, может оказаться в сети с разбиением). Решение заключается в требовании согласия большинства узлов, что обеспечивает как безопасность (согласованность), так и работоспособность (прогресс).

Распределенный консенсус тесно связан с атомарной трансляцией — задачей доставки сообщений всем узлам в одинаковом порядке. Консенсус и атомарная трансляция функционально взаимозаменяемы, поскольку каждый из них может быть реализован с помощью другого. В большинстве практических систем для достижения атомарной трансляции используются алгоритмы консенсуса (хотя некоторые реализации, такие как ZAB в ZooKeeper, позиционируют себя как протоколы атомарной трансляции), обеспечивающие обработку всех реплик в одинаковой последовательности.

## Для чего нужен консенсус?

Прежде чем рассматривать конкретные алгоритмы, необходимо понять суть требований к соглашению. Несмотря на разнообразные проявления, проблемы консенсуса сводятся к двум основным моделям: согласие по одному значению или согласие по последовательности (достигаемое с помощью нескольких экземпляров консенсуса по одному значению). Большинство практических приложений представляют собой реализации этих основных моделей.

### Консенсус по одному значению

Консенсус по одному значению представляет собой простейшую форму, охватывающую простые решения, выбор лидера или определение членства в кластере. Базовый Paxos решает эту задачу, позволяя нескольким узлам согласовать одно конкретное значение.

**Примеры:**
- **Выбор лидера**: какой узел (A, B или C) должен быть лидером?
- **Изменения конфигурации**: следует ли добавить узел D в кластер?
- **Простые решения**: следует ли включить режим обслуживания (да/нет)?
- **Членство в кластере**: текущие активные узлы должны быть \{A, B, C\}
- **Изменения членства**: безопасный переход от \{A, B, C\} к \{A, B, C, D\}

**Алгоритмы:** Здесь превосходит базовый алгоритм Paxos, который был разработан специально для достижения консенсуса по одному значению. Совместный консенсус Raft и варианты Paxos с протоколами членства позволяют обрабатывать более сложные сценарии изменения членства.

### Консенсус по последовательности (полный порядок / реплицированные журналы)

Консенсус последовательности представляет собой наиболее распространенное требование — по сути, это реплицированный журнал, в котором все узлы соглашаются с идентичными упорядоченными списками операций. Этот подход, называемый консенсусом полного порядка, требует, чтобы все узлы обрабатывали операции в идентичной последовательности, и составляет основу большинства распределенных систем.

**Примеры:**
- **Транзакции базы данных**: `[INSERT user Alice, UPDATE account balance, DELETE old record]`
- **Репликация состояний машины**: `[SET x=1, SET y=2, DELETE z]`
- **Потоки событий**: `[OrderCreated, PaymentProcessed, OrderShipped]`
- **Блокчейн**: блоки должны быть в одном порядке на всех узлах
- **Исходные события**: события должны быть упорядочены последовательно для всех потребителей

**Алгоритмы:** Multi-Paxos и Raft разработаны специально для этой цели — они могут эффективно согласовывать множество значений в последовательности, обеспечивая при этом полную упорядоченность.

### Как алгоритмы соответствуют требованиям

Различные алгоритмы консенсуса оптимизированы для разных сценариев:

**Basic Paxos**: идеально подходит для решений с одним значением, но неэффективен для последовательностей. Для каждого решения необходимо запускать отдельный экземпляр Paxos.
**Multi-Paxos**: оптимизирует Paxos для последовательностей, выбирая стабильного лидера, который может предлагать несколько значений без повторения фазы подготовки.

**Raft**: разработан с нуля для реплицированных журналов. Модель сильного лидера делает консенсус по последовательности естественным и эффективным.

**PBFT (Byzantine Paxos)**: расширяет консенсус для обработки вредоносных узлов, что крайне важно для блокчейнов и многоорганизационных систем.

**ZAB (ZooKeeper Atomic Broadcast)**: разработан специально как протокол атомарной трансляции, обеспечивающий полную упорядоченность доставки сообщений.

### Подход на основе конечных автоматов

Большинство современных распределенных систем используют консенсус для построения реплицированных конечных автоматов:

1. **Клиенты отправляют команды** (SET x=5, DELETE user Alice)
2. **Алгоритм консенсуса упорядочивает команды** во всех репликах
3. **Каждая реплика применяет команды** в согласованном порядке
4. **Все реплики оказываются в одинаковом состоянии**

Этот паттерн работает независимо от того, создаете ли вы распределенную базу данных, хранилище конфигураций или службу координации. Алгоритм консенсуса гарантирует, что все реплики видят одну и ту же последовательность операций, а логика состояния машины определяет, что на самом деле делают эти операции.

## Paxos: теоретические основы

Paxos, представленный Лесли Лампортом в 1989 году, был первым практическим решением для распределенного консенсуса. Он математически элегантен, но известен своей сложностью для понимания и правильной реализации. Несмотря на свою сложность, Paxos лежит в основе некоторых крупнейших в мире распределенных систем, включая Google Spanner, службу блокировки Chubby, и используется в облегченных транзакциях Apache Cassandra.

### Основная идея

Paxos работает так, что узлы берут на себя разные роли в тщательно отрепетированном танце предложений и обещаний. Ключевая идея заключается в использовании двухфазного протокола: сначала подготавливают почву, получая обещания от большинства (обещания игнорировать любые будущие предложения с более низкими номерами), а затем предлагают значение, которое соответствует этим обещаниям.

### Роли в Paxos

- **Предлагающий**: инициирует процесс консенсуса, предлагая значения
- **Принимающий**: голосует за предложения и обещает игнорировать более старые предложения
- **Учащийся**: узнает выбранное значение после достижения консенсуса

Один узел может играть несколько ролей одновременно.

### Рабочий процесс Paxos
Процесс начинается, когда клиент отправляет запрос любому узлу в кластере (например, «установить конфигурацию X» или «выбрать лидера Y»). Этот узел становится предлагающим и инициирует двухфазный протокол консенсуса, чтобы все узлы согласились с значением, предложенным клиентом.
**Фаза 1 — Подготовка:**
1. Предлагающий генерирует уникальный номер предложения N
2. Отправляет «prepare(N)» большинству принимающих
3. Каждый принимающий отвечает:
- Обещанием игнорировать предложения < N
- Предложение с наибольшим номером, которое он ранее принял (если таковое имеется), чтобы предлагающий знал, какое значение следует учитывать

``` mermaid
sequenceDiagram
participant P as Предлагающий
participant A1 as Принимающий 1
participant A2 as Принимающий 2
participant A3 as Принимающий 3

Note over P, A3: Фаза 1: Подготовка
P->>A1: prepare(5)
P->>A2: prepare(5)
P->>A3: prepare(5)

A1->>P: promise(5)
A2->>P: promise(5, prev=3, «X»)
A3->>P: promise(5)

Note over P: Большинство ответило — можно переходить к фазе 2
```

**Фаза 2 — Принятие:**
1. Если большинство ответило, предлагающий выбирает значение:
- Если принимающие вернули предыдущие предложения, используется значение из предложения с наибольшим номером
- В противном случае используется значение, предложенное предлагающим
2. Отправляется «accept(N, value)» большинству принимающих
3. Принимающие принимают, если они не обещали игнорировать N
4. Как только большинство принимает, значение выбирается

``` mermaid
sequenceDiagram
participant P as Предлагающий
participant A1 as Принимающий 1
participant A2 as Принимающий 2
participant A3 as Принимающий 3

Note over P, A3: Фаза 2: Принятие
Note over P: Использует «X» из предыдущего предложения с наибольшим номером
P->>A1: accept(5, «X»)
P->>A2: accept(5, «X»)
P->>A3: accept(5, «X»)

A1->>P: accepted(5, «X»)
A2->>P: accepted(5, «X»)
A3->>P: accepted(5, «X»)

Note over P,A3: Консенсус достигнут: значение «X»
```

### Paxos на практике: Multi-Paxos

Базовый Paxos неэффективен для нескольких значений подряд — для каждого решения необходимо запускать полный протокол. Multi-Paxos оптимизирует этот процесс, выбирая стабильного лидера, который может пропустить фазу 1 для последующих предложений, что делает его практичным для реальных систем.

Multi-Paxos работает так, что узлы соглашаются не только по отдельным значениям, но и по последовательности значений (как в реплицированном журнале). После установления лидера он может предлагать значения для нескольких позиций в журнале, не проходя каждый раз фазу подготовки. Базовый Paxos может гарантировать порядок, если запустить отдельные экземпляры для каждой позиции в последовательности (экземпляр Paxos 1 для записи журнала 1, экземпляр 2 для записи 2 и т. д.), но это неэффективно, поскольку каждый экземпляр требует полного двухфазного протокола.

### Где используется Paxos

- **Google Spanner**: использует Paxos для согласованных глобальных транзакций
- **Apache Cassandra**: легкие транзакции используют Paxos для линеаризуемых операций
- **Google Chubby**: служба распределенных блокировок, построенная на Paxos
- **Amazon DynamoDB**: использует варианты Paxos для межрегиональной согласованности

### Проблемы Paxos

- **Сложность реализации**: легко ошибиться, сложно отлаживать
- **Производительность при конфликтах**: несколько инициаторов могут привести к лайвлокам
- **Понятность**: даже опытные инженеры сталкиваются с крайними случаями
- **Накладные расходы на передачу сообщений**: двухфазный протокол требует большего количества сетевых циклов

## Raft: консенсус для людей

Raft был разработан в 2013 году с конкретной целью: сделать распределенный консенсус понятным. Создатели поняли, что сложность Paxos препятствует внедрению и инновациям в распределенных системах. Raft обеспечивает те же гарантии, что и Paxos, но с дизайном, который инженеры могут действительно понять.

### Основная идея

Raft упрощает консенсус, разбивая его на три независимые подзадачи:
1. **Выбор лидера**: как выбрать координатора
2. **Репликация журнала**: как лидер поддерживает согласованность
3. **Безопасность**: обеспечение согласованности системы даже во время сбоев

Ключевая идея заключается в наличии сильного лидера, который координирует все изменения, устраняя необходимость в нескольких конкурирующих инициаторах.

### Роли и состояния Raft

- **Лидер**: обрабатывает запросы клиентов и координирует репликацию
- **Последователь**: пассивно принимает обновления от лидера
- **Кандидат**: стремится стать лидером во время выборов

### Выбор лидера

Когда лидер выходит из строя или становится недоступным, или когда система запускается, Raft обеспечивает быстрое избрание нового лидера кластером. При запуске или когда последователи обнаруживают сбой лидера по отсутствию сигналов сердцебиения, узлы могут стать кандидатами и запросить голоса у других узлов. Первый кандидат, получивший большинство голосов, становится новым лидером на следующий срок.

``` mermaid
sequenceDiagram
participant A as Узел A (Лидер)
participant B as Узел B (последователь)
participant C as Узел C (последователь)

Note over A, C: Нормальная работа — срок 3
A->>B: пульс (срок 3)
A->>C: пульс (срок 3)
B->>A: ACK
C->>A: ACK

Note over A: Лидер A вышел из строя/разбился на части
A--xB: (нет сердцебиения)
A--xC: (нет сердцебиения)

Note over B, C: Время выборов истекло — B становится кандидатом
B->>B: Увеличить срок до 4, проголосовать за себя
B->>C: RequestVote(Срок 4, candidateId: B)

Note over C: C не получил ответа от лидера, предоставляет голос
C->>B: VoteGranted(Срок 4)

Note over B: Достигнуто большинство (2/3) — B становится лидером
B->>C: сердцебиение (Срок 4, Лидер: B)
C->>B: ACK

Note over B,C: Избран новый лидер на срок 4
```
### Репликация журнала

Лидер получает запросы от клиентов (любой узел может получать запросы, но последователи перенаправляют их лидеру), добавляет их в свой локальный журнал, а затем реплицирует записи в журналы последователей. Только после того, как большинство узлов сохранили запись, лидер фиксирует ее и уведомляет последователей. Это обеспечивает строгую согласованность между всеми узлами.

``` mermaid
sequenceDiagram
participant Client as Клиент
participant Leader as Лидер (B)
participant F1 as Последователь (A)
participant F2 as Последователь (C)

Client->>Leader: Команда «SET x=5»

Note over Leader: 1. Добавить в локальный журнал
Leader->>Leader: Журнал[4] = «SET x=5» (не зафиксировано)

Note over Leader,F2: 2. Отправить последователям
Leader->>F1: AppendEntries(term=4, prevIndex=3, entry="SET x=5")
Leader->>F2: AppendEntries(term=4, prevIndex=3, entry="SET x=5")

Note over F1, F2: 3. Последователи проверяют и добавляют
F1->>Leader: Успех (term=4)
F2->>Leader: Успех (term=4)

Note over Leader: 4. Большинство реплицировано (3/3) — можно фиксировать
Leader->>Leader: commitIndex = 4

Note over Leader,F2: 5. Уведомить о фиксации
Leader->>F1: AppendEntries(term=4, commitIndex=4)
Leader->>F2: AppendEntries(term=4, commitIndex=4)

Leader->>Client: Успешно — команда зафиксирована

Note over Client,F2: Все узлы имеют согласованное состояние фиксации
```

### Безопасность и восстановление

Механизмы безопасности Raft предотвращают повреждение данных при повторном присоединении узлов после разделения. Каждый термин имеет не более одного лидера, а узлы с устаревшей информацией обновляются посредством проверки согласованности журналов. Более высокие термины всегда переопределяют более низкие, обеспечивая сходимость кластера к единому согласованному состоянию.

``` mermaid
sequenceDiagram
participant A as Узел A
participant B as Узел B (новый лидер)
participant C as Узел C

Note over A, C: Узел A воссоединяется после разделения
Note over A: A считает, что он по-прежнему является лидером (старый термин 3)
A->>B: AppendEntries(term=3, ...)

Note over B: B является лидером термина 4 (более высокий термин)
B->>A: Отклонить — существует более высокий термин (term=4)

Note over A: A обнаруживает более высокий термин, становится последователем
A->>A: Обновление термина до 4, становится последователем

Note over B, C: Лидер B отправляет записи журнала, чтобы догнать A
B->>A: AppendEntries(term=4, entries=[...])

Note over A: A проверяет согласованность журнала
alt Журнал совпадает
A->>B: Успех — журнал обновлен
else Конфликт журналов
A->>B: Неудача — несоответствие журналов в индексе X
Note over B: B уменьшает nextIndex для A, повторяет попытку
B->>A: AppendEntries(из предыдущего индекса)
end

Note over A, C: A догоняет и присоединяется к текущему термину
B->>A: пульс (term=4)
A->>B: ACK

Note over A, C: Все узлы синхронизированы в термине 4
```

### Свойства безопасности

Raft гарантирует несколько ключевых свойств безопасности:

- **Безопасность выбора**: не более одного лидера на термин
- **Лидер только для добавления**: лидеры никогда не перезаписывают свои журналы
- **Соответствие журналов**: идентичные записи в одном индексе во всех журналах
- **Полнота лидера**: зафиксированные записи появляются в журналах будущих лидеров
- **Безопасность состояния машины**: один и тот же индекс журнала = одна и та же команда

### Преимущества Raft

- **etcd**: хранилище конфигурации Kubernetes использует Raft для обеспечения согласованности
- **HashiCorp Consul**: координация сервисной сетки с помощью консенсуса Raft
- **CockroachDB**: использует Raft для обеспечения согласованности SQL в разных регионах
- **TiKV**: распределенное хранилище ключей-значений с репликацией на основе Raft

### Модификации Raft

- **Предварительное голосование**: предотвращает ненужные выборы из разбитых на разделы узлов
- **Совместный консенсус**: безопасные изменения членства без потери доступности
- **KRaft (Kafka)**: Apache Kafka заменяет ZooKeeper на Raft для метаданных
- **Multi-Raft**: запуск нескольких независимых групп Raft для горизонтального масштабирования

## Paxos vs. Raft: выбор правильного алгоритма

Оба алгоритма решают одну и ту же фундаментальную проблему, но с разными философиями и компромиссами:

**Обработка запросов клиентов:**
- **Paxos**: любой узел может принимать запросы клиентов и выступать в качестве инициатора, запуская протокол консенсуса
- **Raft**: только лидер принимает запросы клиентов; последователи перенаправляют клиентов к текущему лидеру
  **Выбирайте Paxos, если** вам нужно наиболее общее решение для достижения консенсуса, у вас есть сценарии с высокой степенью конфликтов с несколькими инициаторами или вы расширяете существующие системы на основе Paxos. Теоретически это элегантное решение, но его сложно правильно реализовать. Возможность любого узла принимать запросы может обеспечить лучшее распределение нагрузки и доступность.

**Выбирайте Raft, если** вы придаете приоритет простоте реализации, пониманию командой и производительности разработки. Его сильная модель лидера делает его естественным выбором для большинства случаев использования распределенных систем, таких как базы данных и хранилища конфигураций, хотя он создает единственную точку узкого места во время нормальной работы.

**С точки зрения производительности**, Raft обычно работает лучше во время нормальной работы благодаря сильному лидерству, устраняющему конфликты, в то время как Paxos может обрабатывать несколько инициаторов, но может иметь более низкую пропускную способность из-за конкурирующих предложений. Модель только с лидером в Raft может стать узким местом при высокой нагрузке, в то время как гибкость Paxos сопровождается накладными расходами на координацию.

## Византийская отказоустойчивость: когда узлам нельзя доверять

И Paxos, и Raft предполагают, что узлы выходят из строя в результате сбоя (модель fail-stop) — они либо работают правильно, либо полностью перестают работать. Но что, если узлы могут вести себя злонамеренно, отправляя конфликтующие сообщения или повреждая данные? Это проблема византийской неисправности, названная в честь проблемы византийских генералов, в которой генералы могут быть предателями.

### Когда важна византийская отказоустойчивость

Византийская отказоустойчивость становится критически важной в:
- **Сетях блокчейнов**: где участники могут быть враждебными
- **Мультиорганизационных системах**: Когда нельзя доверять всем участникам
- **Критически важная инфраструктура**: где скомпрометированные узлы могут привести к катастрофическим сбоям
- **Распределенные реестры**: где финансовые стимулы могут мотивировать злонамеренное поведение

### Алгоритмы византийского консенсуса

Алгоритмы византийского консенсуса разработаны для сред, где некоторые узлы могут действовать злонамеренно или быть скомпрометированы. Они требуют большего количества узлов и более сложных сообщений, чем алгоритмы с отказоустойчивостью при сбоях.

#### Традиционный византийский консенсус (сети с разрешенным доступом)

**PBFT (Practical Byzantine Fault Tolerance)** был первым практическим алгоритмом византийского консенсуса. Он требует 3f+1 узлов для толерантности к f византийским сбоям (по сравнению с 2f+1 для сбоев при сбое) и использует трехфазный протокол с криптографической верификацией. Он используется в Hyperledger Fabric (платформа блокчейна для предприятий), библиотеке BFT-SMaRt (Java Byzantine replication framework) и академических исследовательских системах.

**Современные усовершенствования:**
- **HotStuff**: обеспечивает линейную сложность сообщений и лучшую производительность, чем PBFT. Используется в проекте Facebook Diem (прекращенная криптовалюта) и некоторых корпоративных платформах.
- **Tendermint**: основан на PBFT с немедленной окончательностью, оптимизирован для использования в блокчейне. Используется в Cosmos Hub (межсетевой протокол), Terra (экосистема стейблкоинов), Binance Chain (блокчейн криптовалютной биржи) и многих блокчейнов Cosmos SDK.
- **Istanbul BFT**: вариант, совместимый с Ethereum, используемый в частных сетях Ethereum и некоторых корпоративных блокчейн-платформах.

#### Консенсус в блокчейне (сети без разрешений)

Сети без разрешений сталкиваются с дополнительными проблемами, поскольку к ним может присоединиться любой, что требует разных подходов:

- **Правило самой длинной цепочки (Bitcoin)**: простой консенсус, при котором побеждает самая длинная действительная цепочка, с использованием Proof of Work для защиты от атак Сибилла и выбора лидера
- **Gasper (Ethereum 2.0)**: сочетает протоколы Casper FFG и LMD GHOST, используя Proof of Stake для выбора валидатора и экономической безопасности

### Стоимость византийской толерантности

Византийские алгоритмы сопровождаются значительными накладными расходами:
- **Требуется больше узлов**: 3f+1 против 2f+1 для отказоустойчивости при сбоях
- **Более высокая сложность сообщений**: узлы должны проверять поведение друг друга
- **Криптографические накладные расходы**: цифровые подписи и проверка увеличивают задержку
- **Сложность реализации**: гораздо сложнее реализовать правильно, чем алгоритмы отказоустойчивости при сбоях
  Для большинства традиционных распределенных систем стоимость византийской толерантности превышает преимущества, поскольку узлы обычно находятся в пределах одной зоны доверия (одна компания, один центр обработки данных и т. д.).

## Будущее консенсуса

Современные распределенные системы продвигают алгоритмы консенсуса в новых направлениях:

**Гибкий консенсус**: протоколы, которые могут адаптировать свои гарантии согласованности в зависимости от потребностей приложения (EPaxos, варианты PBFT).

**Интеграция блокчейна**: адаптация классического консенсуса для платформ криптовалют и смарт-контрактов.

**Геораспределенные системы**: обработка консенсуса между несколькими центрами обработки данных с различными сетевыми условиями.

Фундаментальная проблема распределенного консенсуса остается актуальной как никогда. По мере того, как мы создаем все более сложные распределенные системы — от архитектур микросервисов до баз данных планетарного масштаба — понимание этих алгоритмов становится необходимым для любого инженера, работающего с распределенными системами.

Выбор между Paxos и Raft часто зависит от приоритетов вашей команды: теоретическая строгость против практической реализации, гибкость против простоты, академическая элегантность против инженерной производительности. Оба алгоритма имеют свое место в наборе инструментов распределенных систем, и понимание обоих сделает вас лучшим инженером распределенных систем.
