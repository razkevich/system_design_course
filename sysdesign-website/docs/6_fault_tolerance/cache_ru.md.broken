# Кэширование: стратегические паттерны реализации и trade-off'ы

## Введение

Кэширование представляет одну из наиболее фундаментальных стратегий оптимизации производительности в распределенных системах, однако его реализация требует тщательного рассмотрения trade-off'ов между производительностью, консистентностью и сложностью. 
В этой статье мы кратко пройдемся по ключевым аспектам обширной темы кэширования, чтобы замотивировать читателя внимательнее относиться к этой задаче, а также наметить точки роста — темы, в которых стоит разобраться глубже.

# А все ли нужно кэшировать?

Начну с неочевидного, а кому-то, возможно, даже глупого вопроса. «Ну конечно нужно», — скажет кто-то и будет неправ. По своей сути, кэширование — это получение заранее подготовленных данных и их более быстрая отдача… при условии, что они есть в кэше.

В этом и кроются две ключевые проблемы:

- Если в кэше нет нужных данных, всё равно придётся получать результат обычным способом, что увеличивает время ответа на время обращения к кэшу плюс время вычисления результата.
- Если данных нет, кто-то должен их туда положить, а это — дополнительная работа.

Эту ситуацию даже можно формализовать.

$AverageTime = DataAccessTime * CacheMissRate + CacheAccessTime$

Разберемся подробнее с переменными:

_AverageTime:_ среднее время ответа от нашего сервиса.

_DataAccessTime:_ среднее время доступа к данным - сколько нужно времени чтобы получить данные без кэша.

_CacheMissRate:_ доля промахов кэша по отношению к общему количеству обращений к кэшу. Она показывает, насколько часто нужные данные отсутствуют в кэше.

$$ \text{Cache Miss Rate} = \frac{\text{Number of Cache Misses}}{\text{Total Number of Cache Accesses}} $$

Те в идеальном случае, если количество промахов кэша равно 0, то CacheMissRate тоже равен нулю. По нашей формуле это означает, что доступ к данным равен времени доступа к кэшу:

$AverageTime = DataAccessTime * 0 + CacheAccessTime = CacheAccessTime$

На практике такое бывает редко.

_CacheAccessTime:_ среднее время доступа к кэшу.

---

Теперь рассмотрим пример. Пусть:

- DataAccessTime = 100ms
- CacheAccessTime = 20ms

Подставим в формулу: $AverageTime=100⋅CacheMissRate+20$

Если кэш не используется, время доступа всегда = 100 ms.

Значит, кэш эффективен, если:

$$ AverageTime (with cache)<DataAccessTime (no cache)=100 \newline

$$

Подставим:

$$ 100⋅CacheMissRate+20<100 $$
$$ 100⋅CacheMissRate<80 $$
$$ CacheMissRate<0.8 $$
Соответственно если CacheMissRate ≥ 0.8, то кэш неэффективен.

Какие выводы из этого можно сделать:

- Не всегда кэширование способно увеличить производительность
- Посчитай метрики своей системы прежде, чем принимать решение о кэшировании

# Механизмы кэширования

Давайте теперь рассмотрим, а что вообще можно кэшировать и где. Для примера возьмем web-приложение, тк этот пример будет близок к большинству читателей.

## Client-Side кэширование

Client-Side кэширование в веб-приложении — это механизм хранения данных (например, HTML, CSS, JS, изображений или даже API-ответов) на стороне клиента (в браузере), чтобы избежать повторных загрузок с сервера.

Это, очевидно, ускоряет загрузку страниц и снижает нагрузку на сервер.

Обычно, такой кэш не требует имплементации и управляется заголовками HTTP:

- `Cache-Control`: основной заголовок, определяет, как и сколько кэшировать.
    - Примеры:
        - `Cache-Control: max-age=3600` — кэшировать на 1 час
        - `Cache-Control: no-cache` — кэш можно хранить, но нужно проверять актуальность на сервере
        - `Cache-Control: no-store` — ничего не кэшировать
- `ETag`: уникальный хеш ресурса, помогает браузеру проверить, изменился ли он.
- `Last-Modified`: дата последнего изменения — тоже для проверки актуальности.

`ETag` (Entity Tag) — это уникальный идентификатор (обычно хеш содержимого файла), который сервер возвращает клиенту в ответ на запрос.

При следующем запросе клиент отправляет этот `ETag` в заголовке `If-None-Match`, и сервер проверяет:

- если содержимое не изменилось — возвращает `304 Not Modified` (без тела)
- если изменилось — возвращает новый контент и новый `ETag`

`Last-Modified` — HTTP-заголовок, который сервер отправляет, указывая дату и время последнего изменения ресурса.

Браузер при повторном запросе отправляет заголовок `If-Modified-Since`, и сервер решает:

- если ресурс не изменился — отвечает `304 Not Modified`
- если изменился ****— отдает новый контент

## CDN

CDN (Content Delivery Network) — это распределённая сеть серверов, которые кэшируют и отдают статический контент пользователю с ближайшего к нему узла.

Типичный пример: статические файлы (изображения, JS, CSS, видео, шрифты) кэшируются на серверах CDN, чтобы:

- ускорить доставку,
- снизить нагрузку на основной сервер (origin),
- обеспечить отказоустойчивость и масштабирование.

Следует заметить, что существуют два основных подхода к тому как контент попадает в CDN - push CDN и pull CDN.

Pull CDN сам "тянет" (pull) контент с origin-сервера по мере необходимости.

Когда пользователь впервые запрашивает ресурс:

1. CDN смотрит: есть ли он в кэше.
2. Если нет — CDN отправляет запрос к origin, получает контент и сохраняет его у себя (кэширует).
3. Последующие запросы отдаются из кэша CDN.

В Push CDN ты вручную заливаешь (push) файлы в CDN через API, FTP, интерфейс или CI/CD. Контент уже хранится на edge-серверах CDN до запроса пользователей.

Какой подход выбрать? Как всегда зависит от, но я попытался сделать простую таблицу-сравнение, которая может помочь.

|Характеристика|**Pull CDN**|**Push CDN**|
|---|---|---|
|Загрузка контента|Автоматически с origin|Вручную или по CI/CD|
|Первое обращение|Медленное (если не в кэше)|Быстрое (контент уже там)|
|Управление файлами|Простое|Требует больше контроля|
|Используется для|Статичных сайтов, API, PWA|Видео, крупных файлов, релизов|

**Популярные CDN-провайдеры**

- **Cloudflare** — бесплатный, простой в использовании
- **Akamai** — энтерпрайз-решения, высокая производительность
- **Amazon CloudFront** — интеграция с AWS
- **Fastly** — продвинутая логика кэширования, edge scripting
- **Vercel / Netlify** — встроенные CDN для фронтенда

## Web-server кэширование

Web server кэширование реализуется на уровне веб-сервера — до того, как запрос попадёт в приложение. Цель — сократить или полностью исключить обработку запросов приложением, если ответ уже сформирован ранее и не изменился.

Допустим, Nginx стоит перед бэкендом и кэширует ответы на `/products/123`.

Если этот ответ был недавно сгенерирован, он уже есть в кэше и Nginx отдаёт его сам, не напрягая приложение.

Подходит для:

- Статических страниц
- Частично динамического контента с длительным TTL
- WordPress, Django и т.п., где HTML генерируется на сервере

Пример конфигурации nginx

```
http {
    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=STATIC:10m max_size=1g inactive=60m use_temp_path=off;

    server {
        listen 80;
        server_name example.com;

        location / {
            proxy_pass <http://backend>;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;

            # Включаем кэширование
            proxy_cache STATIC;
            proxy_cache_valid 200 302 10m;
            proxy_cache_valid 404 1m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            add_header X-Cache-Status $upstream_cache_status;
        }
    }
}
```

Пояснение:

|Строка|Что делает|
|---|---|
|`proxy_cache_path`|Определяет, где хранится кэш, объём, время хранения, зону кэша|
|`proxy_pass <http://backend`>|Прокси-передача на backend-приложение (например, Node.js, Java)|
|`proxy_cache STATIC`|Использует зону `STATIC` (определена выше)|
|`proxy_cache_valid`|Кэширует ответы: 200, 302 — на 10 минут; 404 — на 1 минуту|
|`proxy_cache_use_stale`|При ошибках отдаёт старую версию, если она есть|
|`add_header X-Cache-Status`|Добавляет заголовок, чтобы видеть результат: `HIT`, `MISS`, `BYPASS`|

## Application-service кэширование

Application caching работает внутри кода приложения. Чаще всего это:

- in-memory кэш (например, `Map`, `ConcurrentHashMap`, `Guava Cache`)
- распределённый кэш (например, Redis, Memcached)

Приложение само определяет, что, когда и как кэшировать.

## DB кэширование

Здесь стоит отметить, что все современные БД сами много чего кэшируют под капотом (Buffer Pool / Page Cache / Query Plan Cache и тд), но в этой статье это рассматриваться не будет. Вместо этого, предлагаю вспомнить о кэшировании часто запрашиваемых данных или результатов запросов.

Этот механизм может быть реализован на двух уровнях.

### **Кэширование на уровне запросов (Query-level caching)**

Кэширование на уровне запросов сохраняет в памяти результаты часто выполняемых SQL-запросов. Когда тот же самый запрос выполняется повторно, возвращается результат из кэша, а не осуществляется новый запрос к базе данных. Это снижает нагрузку на БД и ускоряет отклик системы.

### **Кэширование на уровне объектов (Object-level caching)**

Кэширование на уровне объектов сохраняет отдельные объекты данных или записи, полученные из базы данных. Этот подход полезен, когда определённые объекты запрашиваются часто или когда данные в базе относительно стабильны. Он снижает необходимость в частых обращениях к базе данных и повышает общую производительность приложения.

## Вывод

Механизм кэширования многослойный. Каждый слой важен и защищает следующий от необходимости обрабатывать запрос. Грамотная дизайн многослойного кэширования ключ к производительности и масштабируемости.

![cache1](/img/cache1.png)

# Инвалидация кэша

Инвалидация — ключевой компонент всей архитектуры кэширования. Недостаточно положить данные в кэш — надо уметь их корректно удалять или обновлять. Особенно если они изменились.

Рассмотрим подходы к инвалидации и их применимость на каждом из уровней.

## 1. TTL

**TTL (Time To Live)** — это время жизни закэшированного значения. По истечении этого срока данные считаются устаревшими и должны быть обновлены или удалены.

TTL задаётся в момент помещения данных в кэш либо берётся из значения по умолчанию.

Оптимальное значение TTL зависит от:

- характера данных,
- требуемой свежести,
- стоимости обновления,
- уровня кэширования.

Если TTL слишком мал, данные быстро перестанут быть валидными, что в большинстве случаев приведёт к высокому **Cache Miss Rate**.

Если TTL слишком велик, пользователь долго будет получать устаревшие данные, что в некоторых бизнес-сценариях может быть критично.

Есть и менее очевидная проблема — **одинаковый TTL для всех записей**. Если срок жизни истечёт у большого количества ключей одновременно, в бэкенд могут полететь тысячи запросов за обновлением данных. Это явление называется **cache stampede**. Чтобы его избежать, используют **jitter** — рандомизацию TTL в рамках заданного диапазона (например, TTL ± 10%). Это помогает распределить нагрузку во времени.

Ещё одна похожая ситуация — **Thundering Herd Problem**. Она возникает, когда множество запросов одновременно обращаются к ключу, срок жизни которого истёк. Все эти запросы начинают параллельно вычислять одно и то же значение для кэша, что может резко снизить производительность системы. Если вы сталкиваетесь с этой проблемой, стоит рассмотреть альтернативные стратегии управления временем жизни и обновлением данных в кэше.

|Компонент|TTL|Типичный TTL|Что кэшировать|Jitter|Особенности|
|---|---|---|---|---|---|
|**Браузер**|Через HTTP-заголовки (`Cache-Control`, `Expires`)|5 минут – несколько дней|Статика: CSS, JS, изображения; иногда API-ответы|Не применяется||
|**CDN**|Через `Cache-Control`, можно задать принудительно|5 минут – несколько часов|HTML, JSON, media-файлы|Обычно не нужен||
|**Proxy сервер**|Настраивается вручную|1–30 минут|HTML, API-ответы, auth-токены|Опционально|Можно использовать стратегию `stale-while-revalidate`|
|**Application сервер**|Полностью настраиваемый|От секунд до дней|Результаты из БД, вычисления, авторизации, сессии|**Рекомендуется**|Особенно важен jitter при массовом чтении кэшируемых данных|

### Рекомендации

- Не кэшируйте то, что легко и быстро пересчитать.
- Используйте **lazy loading** или **write-through cache**, если важно не терять согласованность.
- TTL + jitter + мониторинг — ваш основной инструмент в борьбе с нагрузкой и устаревшими данными.
- Следите за **hit ratio** и **latency**: они подскажут, хорошо ли вы подобрали TTL.

## 2. Инвалидация по событию

Инвалидация кэша по событию — это механизм, при котором кэш очищается или обновляется в момент изменения данных, а не по истечении TTL.

Когда источник данных (например, БД или API) обновляется, в систему отправляется событие, которое удаляет или обновляет соответствующую запись в кэше.

**Примеры событий:**

- Обновление профиля пользователя
- Удаление товара
- Изменение настроек фичи
- Сохранение нового отчёта

### Варианты инвалидации

1. **Удаление (invalidate / evict)** — при изменении данных связанные записи удаляются из кэша. При следующем запросе произойдёт повторная загрузка данных из источника.
2. **Обновление (refresh)** — если новое значение известно в момент события, кэш можно сразу обновить, минуя этап повторного запроса к источнику.

На практике чаще встречается стратегия **refresh**, однако в некоторых случаях удаление (**invalidate**) может быть предпочтительнее — например, если обновление данных в кэше слишком затратное или риск получения неконсистентного значения велик.

### Варианты доставка события

|Подход|Механизм действия|Пример использования|Технологии / Инструменты|
|---|---|---|---|
|**Прямой синхронный вызов**|При обновлении объекта приложение сразу вызывает инвалидацию кэша|Обновление профиля пользователя сразу сбрасывает кэш|—|
|**Асинхронный вызов (Pub/Sub)**|Сервис A обновляет данные и публикует событие `UserUpdated(id=123)`Сервис B инвалидирует кэш при получении события|Обновление пользователя — событие инвалидирует кэш в стороннем сервисе|Kafka, RabbitMQ, Redis PubSub, AWS SNS/SQS|

### Где применяют

|Слой|Используют?|Пример|
|---|---|---|
|Браузер|⚠️ Частично|Обычно нет, возможно в Progressive Web App|
|CDN|⚠️ Частично|Через purge-запросы (API Cloudflare, Akamai)|
|Web Server|⚠️ Частично|Через внешние скрипты или purge|
|App-level|✅ Да|Redis, Guava, Spring Cache, etc|
|БД|✅ Иногда|Триггеры, CDC → отправка событий в очередь|

## Версионирование кэша

**Версионирование кэша** — это приём, при котором в ключ (или URL) кэш-записи добавляется идентификатор версии. Когда выходит новая версия приложения или данных, этот идентификатор меняется, и браузер, CDN или другой слой кэширования воспринимает запись как новую, запрашивая её заново.

### Что такое версия и куда её вставлять

Версия — это любая метка, которая гарантированно меняется, когда меняется контент:

- хеш содержимого (`app.a8c3f7.js`);
- номер билда (`/static/20250805/app.js`);
- query-параметр (`?v=2025-08-05`);
- префикс ключа в Redis (`v3:user:42`);
- имя bucket’а в CacheStorage (`my-site-v4`).

Важно не то, как именно она выглядит, а то, что у разных версий разный ключ.

Выбор подхода зависит от специфики системы: в вебе важнее хеши и URL-версии, в API — версионирование эндпоинтов, в in-memory кэшах — версии в ключах.

Грамотно настроенное версионирование снижает нагрузку, улучшает пользовательский опыт и упрощает поддержку проекта.

## Тэгирование кэша

Тегирование кэша — это метод организации и управления кэшированными данными с помощью тегов, которые позволяют логически группировать и эффективно инвалидировать связанные записи кэша.

Вместо того чтобы кэшировать данные по одиночным ключам и инвалидировать каждую запись вручную, тегирование позволяет присваивать нескольким записям общие теги и удалять их массово, ссылаясь на этот тег.

## Зачем нужно тегирование кэша

### 1. Массовая и точная инвалидизация

Когда данные изменяются, нужно быстро очистить только те кэш-записи, которые к ним относятся. Примеры:

- Обновился пользователь — нужно сбросить все кэш-записи, связанные с этим пользователем.
- Изменилась статья — нужно сбросить кэш страницы статьи и связанных данных (комментарии, метаинформация и т.д.).

### 2. Логическая группировка данных

Тегирование помогает лучше структурировать кэш. Например:

- Тег `user:123` может быть применён к профилю, списку заказов и настройкам пользователя.
- Тег `product:456` — к карточке товара, обзорам, и результатам поиска с этим товаром.

### 3. Повышение эффективности

Тегирование минимизирует ошибки инвалидизации и повышает производительность — не нужно искать и перебирать кэш-ключи вручную.

# Алгоритмы вытеснения данных

Помимо алгоритмов инвалидации, одна из основных проблем в кэшировании - это вытеснение данных в кэше, так как размер кэша очевидно ограничен.

При переполнении необходимо удалить (вытеснить) часть данных, чтобы освободить место для новых. Алгоритмы вытеснения определяют, какие данные будут удалены, и оказывают критическое влияние на производительность системы.

Это не такая простая проблема, как может показаться на первый взгляд. Удалить данные просто, но как при этом максимизировать возможный cache hit и уменьшить cache miss.

Рассмотрим основные подходы.

## 1. FIFO (First-In, First-Out)

**Принцип**: первым пришёл — первым ушёл. Самые старые элементы удаляются первыми.

**Плюсы**:

- Простая реализация.
- Подходит, если порядок поступления данных важнее их актуальности.

**Минусы**:

- Игнорирует частоту и давность использования.
- Может вытеснять часто используемые данные.

**Применение**: когда простота важнее производительности, например в ограниченных в ресурсах встраиваемых системах.

## 2. LRU (Least Recently Used)

**Принцип**: вытесняется наименее недавно использованный элемент. Он происходит из идеи, что часто если к данным давно не обращались, то они могут быть удалены.

**Плюсы**:

- Хорошая производительность в типичных сценариях доступа.
- Основан на предположении, что недавно использованные данные вероятнее будут востребованы снова.

**Минусы**:

- Более сложная реализация (например, требуется двусвязный список и хеш-таблица).
- Может плохо работать с циклическим доступом к данным большого объема.

**Применение**: кэш браузеров, базы данных, файловые системы.

## 3. LFU (Least Frequently Used)

**Принцип**: удаляется элемент, который использовался наименее часто. Аналогично LRU, но учитывает частоту данных а не давность доступа.

**Плюсы**:

- Учитывает популярность данных.
- Хорошо работает при стабильных паттернах доступа.

**Минусы**:

- Требует учета количества обращений, что усложняет реализацию.
- Может сохранять устаревшие, но ранее популярные данные.

**Варианты улучшения**:

- **LFU with Aging** — периодическое уменьшение счётчиков. Каждый период времени (или при каждом доступе) счётчики всех элементов уменьшаются на определённую величину. Таким образом, старые и давно не используемые элементы постепенно «теряют вес» и могут быть вытеснены.

**Применение**: кеширование в долгоживущих сервисах, где паттерны доступа стабильны, а цель — держать в кэше те элементы, которые используются чаще _всего_, а не просто недавно.

## 4. MRU (Most Recently Used)

**Принцип:** Удаляется **наиболее недавно** использованный элемент. Это полезно в сценариях, где если данные только что использовались, то с высокой вероятностью они не понадобятся в ближайшее время. То есть, паттерн доступа противоположен тому, на который рассчитан LRU.

**Плюсы:**

- Эффективен при последовательном доступе к данным (после использования элемент редко нужен снова).
- Быстро освобождает кэш для новых данных в потоковых и batch-сценариях.
- Может снижать избыточное хранение ненужных данных при линейной обработке больших файлов или наборов.

**Минусы:**

- Плохо работает при временной локальности (часто удаляет данные, которые скоро понадобятся).
- Сильно зависит от паттерна доступа, в универсальных сценариях даёт низкий cache hit.
- Неэффективен при циклической или повторяющейся обработке данных.
- Часто снижает производительность по сравнению с LRU в типичных приложениях.
- Может увеличивать нагрузку на I/O из-за большего числа промахов кэша.

**Применение**:

- **Последовательный доступ** к данным:
    - Потоковое чтение файлов (например, медиафайлы, большие базы данных при экспорте).
    - Пакетная обработка (batch-processing), где данные используются один раз.
- **Буферы ввода-вывода** в СУБД для временных таблиц и сканов.
- **Обработка больших наборов данных**, когда каждый элемент используется лишь короткое время, а потом больше не нужен.
- **ETL-процессы** (Extract-Transform-Load), где данные проходят этапы и больше не возвращаются в предыдущий.

## 5. Random Replacement (RR)

**Принцип**: удаляется случайный элемент.

**Плюсы**:

- Очень простая реализация.
- Избегает «паттернов залипания» и не требует сложных структур данных.

**Минусы**:

- Непредсказуемая производительность.
- Возможна частая замена полезных данных.

**Применение**: системы с ограниченными вычислительными ресурсами (например, в аппаратном обеспечении).

## 6. Second Chance

**Принцип**: модифицированная версия FIFO, где каждый элемент при вытеснении получает «второй шанс». Вместо немедленного удаления проверяется бит использования: если он установлен, элемент перемещается в конец очереди и бит сбрасывается; если нет — элемент удаляется.

**Плюсы**:

- Простая реализация при небольшой доработке FIFO.
- Учитывает, использовался ли элемент недавно, что повышает cache hit по сравнению с чистым FIFO.
- Баланс между простотой и учётом актуальности данных.

**Минусы**:

- Не учитывает частоту использования, только факт последнего доступа.
- При большом числе активных элементов все могут получать «второй шанс», что снижает эффективность.
- Чуть сложнее в реализации, чем чистый FIFO (нужен бит использования).

**Применение**:

- Операционные системы для управления страницами памяти (в том числе в UNIX/Linux).
- Кэширование в системах с ограниченными ресурсами, где нужно немного улучшить FIFO без внедрения сложных алгоритмов.
- Буферы ввода-вывода в файловых системах.

## 7. **Clock (часовой)**

**Принцип**: оптимизированная реализация Second Chance, где элементы расположены по кругу, а указатель («стрелка часов») проходит по ним. При вытеснении проверяется бит использования: если он равен 0 — элемент удаляется; если 1 — бит сбрасывается, и указатель двигается дальше, пока не найдёт кандидат на удаление.

**Плюсы**:

- Экономит память по сравнению с классическим Second Chance (нет необходимости перемещать элементы в очередь).
- Эффективен по времени — обход идёт циклически, без перестроения структуры.
- Простая и быстрая реализация при большом числе элементов.

**Минусы**:

- Как и Second Chance, учитывает только факт недавнего использования, но не частоту.
- При большом числе активных страниц может долго искать жертву на вытеснение.
- Работает хуже в сценариях с быстрым и случайным доступом, где многие элементы имеют бит 1.

**Применение**:

- Управление виртуальной памятью в ОС (Windows, Linux).
- Буферы страниц в СУБД (PostgreSQL, Oracle).
- Кэш дисковых блоков в файловых системах.

## 8. **2Q (Two Queues)**

**Принцип**: алгоритм использует две очереди — одна для новых элементов (FIFO), вторая для часто используемых (LRU). Новый элемент сначала попадает в очередь FIFO; если он используется повторно, то перемещается в очередь LRU. При переполнении вытеснение происходит из FIFO или LRU в зависимости от настроек.

**Плюсы**:

- Учитывает и актуальность, и частоту использования.
- Избегает проблем LRU с «разовым шумом» — случайно загруженные и больше не нужные данные быстро вытесняются.
- Гибко настраивается под разные паттерны доступа.

**Минусы**:

- Сложнее в реализации, чем LRU или FIFO.
- Требует настройки размера каждой очереди для оптимальной работы.
- При неверной конфигурации может работать хуже, чем более простые алгоритмы.

**Применение**:

- Кэш в базах данных (например, в PostgreSQL для буфера страниц).
- Файловые системы и дисковые кэши, где важно отсеивать «одноразовые» данные.
- Веб-серверы и прокси-сервера с высокой нагрузкой и смешанными паттернами запросов.

## 9. **SLRU (Segmented LRU)**

**Принцип**: кэш делится на два сегмента — **probationary** (испытательный) и **protected** (защищённый). Новый элемент сначала попадает в испытательный сегмент, работающий по LRU. Если элемент повторно используется, он перемещается в защищённый сегмент (также LRU). При переполнении вытеснение сначала идёт из испытательного сегмента.

**Плюсы**:

- Хорошо отсекает «одноразовые» данные.
- Часто используемые элементы дольше сохраняются в кэше.
- Учитывает и частоту, и актуальность использования.
- Более предсказуемое поведение по сравнению с классическим LRU при смешанных нагрузках.

**Минусы**:

- Сложнее в реализации, чем LRU или FIFO.
- Требует настройки размера сегментов для оптимальной производительности.
- При неправильной конфигурации может вести себя как обычный LRU, теряя преимущества.

**Применение**:

- Файловые системы (например, в ZFS).
- Кэш в базах данных (PostgreSQL, Oracle).
- Прокси-серверы и CDN, где важно защитить «долгоживущие» популярные данные от вытеснения случайным трафиком.

## 10. **TLRU (Time Aware LRU)**

**Принцип**: модификация LRU, в которой у каждого элемента есть время жизни (TTL). При вытеснении или обращении алгоритм учитывает не только давность использования, но и оставшееся время до истечения TTL. Элементы с истёкшим сроком жизни удаляются первыми, даже если они недавно использовались.

**Плюсы**:

- Позволяет учитывать срок актуальности данных.
- Предотвращает хранение устаревшей информации в кэше.
- Хорошо работает для данных с известным временем устаревания (например, цен или котировок).

**Минусы**:

- Требует хранения и проверки дополнительного метаданных (TTL).
- Увеличивает сложность реализации по сравнению с LRU.
- Может преждевременно вытеснять данные, которые ещё могли быть полезны, если TTL задан слишком жёстко.

**Применение**:

- Финансовые системы для кэширования цен, ставок, курсов валют.
- CDN и веб-кэши, где ресурсы имеют явный срок действия.
- IoT и телеметрия, когда данные теряют ценность через определённый промежуток времени.

## 11. **LRU-K**

**Принцип**: расширение LRU, которое учитывает не только последний доступ, но и **K-й последний доступ** к элементу. Для каждого элемента ведётся история обращений, и при вытеснении выбираются те, у которых K-й последний доступ был наиболее давно. Это помогает отличать случайные разовые обращения от стабильно часто используемых данных.

**Плюсы**:

- Эффективно отсекает «одноразовый шум» в кэше.
- Лучше адаптируется к реальным паттернам использования, чем классический LRU.
- Может хранить в кэше элементы, которые используются с определённой регулярностью, даже если между обращениями большие интервалы.

**Минусы**:

- Более сложная реализация и выше требования к памяти (хранение истории обращений).
- Необходима настройка параметра K — при неправильном выборе алгоритм может терять эффективность.
- Увеличивает накладные расходы на обновление статистики при каждом обращении.

**Применение**:

- СУБД (например, Oracle, PostgreSQL) для управления буфером страниц.
- Файловые системы и дисковые кэши, где важно удерживать часто используемые, но не обязательно «свежие» данные.
- Сценарии, где встречается повторное использование с большими паузами (научные расчёты, аналитические запросы).

## 12. ARC (Adaptive Replacement Cache)

**Принцип**: адаптивное сочетание LRU и LFU. Кэш делится на четыре списка:

- **T1** — недавно использованные элементы (аналог LRU).
- **T2** — часто используемые элементы (аналог LFU).
- **B1** — «призраки» недавно вытесненных элементов из T1 (только метаданные).
- **B2** — «призраки» часто вытесненных элементов из T2.

Алгоритм динамически регулирует размер T1 и T2, исходя из того, из какого списка чаще возвращаются элементы при повторных запросах.

**Плюсы**:

- Сам адаптируется к типу нагрузки (больше LRU или больше LFU).
- Учитывает и актуальность, и частоту использования.
- Эффективен при изменяющихся паттернах доступа.
- Гарантирует производительность не хуже LRU в худшем случае.

**Минусы**:

- Более сложная реализация и высокая накладная стоимость по сравнению с LRU.
- Требует хранения дополнительной метаинформации (призрачные списки).
- Для очень маленьких кэшей накладные расходы могут перевесить выгоду.

**Применение**:

- СУБД (например, IBM DB2, PostgreSQL через расширения).
- Системы хранения данных и файловые системы (ZFS, Lustre).
- Высоконагруженные веб-кэши и прокси, где нагрузка меняется во времени.

## 13. LIRS (Low Inter-reference Recency Set)

**Принцип**: алгоритм, который улучшает LRU, анализируя **расстояние между повторами обращений** (reuse distance) и разделяя элементы на:

- **LIR** (Low Inter-reference Recency) — часто используемые с маленьким интервалом между обращениями, остаются в кэше.
- **HIR** (High Inter-reference Recency) — элементы с большим интервалом между обращениями, хранятся в кэше только временно.

LIRS динамически перемещает элементы между LIR и HIR в зависимости от реальных паттернов доступа.

**Плюсы**:

- Сильно снижает влияние «одноразового шума» по сравнению с LRU.
- Удерживает данные с реальной повторной востребованностью даже при больших интервалах между обращениями.
- Эффективнее LRU в сценариях с большим рабочим набором данных и сложными паттернами доступа.

**Минусы**:

- Сложнее в реализации, чем LRU, требует поддержания двух структур (стека и очереди).
- Более высокие накладные расходы на обновление структур при каждом обращении.
- Не всегда даёт выигрыш в сценариях с простыми паттернами (где LRU уже оптимален).

**Применение**:

- Системы хранения данных и СУБД (MySQL, PostgreSQL) для оптимизации буфера страниц.
- Файловые системы с высокими требованиями к производительности (например, Linux VFS).
- Высоконагруженные кэши в веб-приложениях и CDN, где трафик непредсказуем и содержит шум.

### 14. **OPT (Optimal Replacement, aka Belady’s Algorithm)**

**Принцип**: теоретически оптимальный алгоритм вытеснения, который всегда удаляет элемент, **который понадобится дальше всего в будущем** (или не понадобится вовсе). Работает, зная полный порядок будущих обращений к данным, поэтому в реальных системах используется только для моделирования и оценки эффективности других алгоритмов.

**Плюсы**:

- Гарантирует минимальное возможное количество cache miss.
- Служит эталоном для сравнения других алгоритмов.
- Помогает в исследовании и настройке кэш-алгоритмов.

**Минусы**:

- Невозможен для реализации в реальном времени (нужно знать будущее).
- Может быть использован только в симуляциях, профилировании или оффлайн-анализе.
- Не учитывает реальных накладных расходов на поддержку структур данных.

**Применение**:

- Моделирование и тестирование алгоритмов кэширования.
- Анализ и сравнение стратегий в научных исследованиях.
- Обучающие материалы для понимания пределов эффективности кэширования.

## Сводная таблица

|Алгоритм|Частота|Давность|TTL|Сложность|Примечания|
|---|---|---|---|---|---|
|FIFO|✘|✘|✘|Простая|Прост, но слеп к актуальности|
|LRU|✘|✔|✘|Средняя|Широко применим|
|LFU|✔|✘|✘|Средняя|Часто сохраняет «старое»|
|MRU|✘|✔|✘|Простая|Уникальные случаи применения|
|RR|✘|✘|✘|Простая|Минимальные ресурсы|
|Second Chance|✘|✔|✘|Средняя|Улучшение FIFO|
|Clock|✘|✔|✘|Средняя|Эффективен по памяти|
|2Q|✔|✔|✘|Сложная|Устойчив к циклам|
|SLRU|✔|✔|✘|Сложная|Сегментированный LRU|
|TLRU|✘|✔|✔|Средняя|Учет времени жизни|
|LRU-K|✔|✔|✘|Сложная|Учет нескольких обращений|
|ARC|✔|✔|✘|Высокая|Адаптивный и сбалансированный|
|LIRS|✔|✔|✘|Высокая|Высокая точность|
|OPT|✔|✔|✘|Теория|Эталон, не реализуем|

# Стратегии кэширования

Выбор правильного алгоритма вытеснения — лишь часть задачи. Не менее важно определить стратегию кэширования, то есть то, как и когда данные попадают в кэш и синхронизируются с основным хранилищем.

Оптимальная стратегия напрямую зависит от характера нагрузки. В реальных системах паттерны доступа к данным могут различаться кардинально:

- В одних случаях преобладают **чтения** (**read-intensive**), и главная цель — минимизировать задержки при выдаче данных, максимально увеличив cache hit.
- В других — большую часть операций составляют записи (**write-intensive**), и на первый план выходят согласованность данных и минимизация издержек на синхронизацию с хранилищем.

Понимание того, какой тип нагрузки доминирует, позволяет выбрать стратегию, которая обеспечит оптимальный баланс между производительностью, консистентностью и стоимостью операций. Далее мы рассмотрим, как различные подходы работают в условиях преимущественно чтения или записи.

![cache2](/img/cache2.png)

## Read-intensive

Когда в системе преобладают операции чтения, основная цель — максимизировать количество cache hit и снизить задержки при получении данных. Здесь используются стратегии, которые позволяют быстро доставлять актуальную информацию при минимальном количестве обращений к основному хранилищу.

### Cache-aside

**Принцип**: приложение само управляет кэшем. При запросе данных сначала идёт проверка в кэше; если данных нет (cache miss) — они загружаются из хранилища, помещаются в кэш, а затем возвращаются пользователю.

**Плюсы**:

- Простая реализация.
- Гибкость: приложение само решает, что и когда кэшировать.
- Легко интегрировать в существующую архитектуру.

**Минусы**:

- При первом запросе к новым данным всегда будет cache miss.
- Приложение должно явно управлять обновлением и удалением данных в кэше.

**Применение**: веб-приложения, API-шлюзы, сервисы с непостоянными наборами данных.

### Read-through

**Принцип**: приложение всегда обращается к кэшу, а тот при необходимости сам загружает данные из хранилища и помещает их в себя.

**Плюсы**:

- Упрощает код приложения — логика загрузки скрыта в кэше.
- Гарантирует, что данные будут помещены в кэш автоматически при первом запросе.

**Минусы**:

- Более сложная реализация кэширующего слоя.
- Может быть сложнее контролировать стратегию обновления.

**Применение**: системы с частыми повторными чтениями, где важно минимизировать cache miss без сложной логики в приложении.

### Refresh-ahead

**Принцип**: кэш сам обновляет данные заранее, до истечения TTL, если прогнозируется, что они скоро понадобятся.

**Плюсы**:

- Снижает вероятность cache miss для часто запрашиваемых данных.
- Пользователи получают свежие данные без задержки на загрузку из хранилища.

**Минусы**:

- Может загружать лишние данные, которые в итоге не будут использованы.
- Требует механизма прогнозирования востребованности данных.

**Применение**: CDN, новостные порталы, торговые платформы, системы с предсказуемыми пиками запросов.

## Write-intensive

Когда в системе преобладают операции записи, ключевая задача — обеспечить **согласованность данных между кэшем и хранилищем**, минимизируя при этом задержки и нагрузку на основное хранилище. Выбор стратегии зависит от того, что критичнее: надёжность и консистентность или скорость записи.

### Write-through

**Принцип**: запись сначала идёт в кэш, а затем синхронно в основное хранилище.

**Плюсы**:

- Высокая согласованность данных.
- Простая логика восстановления после сбоя — данные уже в хранилище.

**Минусы**:

- Запись медленнее, так как идёт синхронно в два места.
- Увеличенная нагрузка на хранилище.

**Применение**: финансовые системы, учётные базы, критичные к потере данных приложения.

### Write-around

**Принцип**: запись идёт напрямую в хранилище, минуя кэш; кэш обновляется только при чтении данных.

**Плюсы**:

- Снижение нагрузки на кэш при массовых записях.
- Предотвращает вытеснение полезных данных из кэша данными, которые могут не понадобиться.

**Минусы**:

- При последующем чтении нового элемента будет cache miss.
- Данные в кэше могут быть устаревшими до первого чтения.

**Применение**: системы с редким повторным чтением недавно записанных данных, логгирование, потоковая запись.

### Write-back

**Принцип**: запись идёт только в кэш, а в основное хранилище данные отправляются асинхронно.

**Плюсы**:

- Высокая скорость записи.
- Снижение нагрузки на хранилище при множественных обновлениях одного и того же элемента.

**Минусы**:

- Риск потери данных при сбое кэша до синхронизации с хранилищем.
- Более сложная реализация механизма синхронизации.

**Применение**: высоконагруженные системы, временные буферы, сценарии с частыми обновлениями одного набора данных.

# Вывод

Кэширование — это не магическая кнопка «ускорить», а инженерная дисциплина с множеством уровней, стратегий и алгоритмов, каждая из которых должна подбираться под конкретный контекст.

Ошибочный подход «просто закэшируем» часто приводит к обратному эффекту: промахи кэша увеличивают задержки, устаревшие данные ломают бизнес-логику, а неудачные алгоритмы вытеснения или стратегии записи — снижают производительность.

Эффективная архитектура кэширования требует:

- понимания характера нагрузки (read-intensive, write-intensive или смешанной);
- выбора подходящей стратегии попадания и инвалидации данных;
- настройки алгоритмов вытеснения с учётом паттернов доступа;
- грамотного TTL, тегирования и, при необходимости, версионирования.

Правильный дизайн кэширования — это баланс между скоростью, консистентностью и ресурсами. И чем раньше этот баланс будет найден в проекте, тем меньше шансов, что кэш станет узким местом, а не инструментом ускорения.

В идеале, кэш должен быть **прозрачным, предсказуемым и измеримым**, а решение о его использовании — результатом анализа метрик, а не интуиции. Именно тогда он перестанет быть «ещё одной оптимизацией» и станет полноценным элементом архитектуры.